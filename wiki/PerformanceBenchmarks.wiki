#summary Summary of performance benchmarks.

= Introduction =
Several libraries were tested for runtime performance in several different operations.  The tested operations were primarily selected based on how commonly they are used or to show off the potential advantages or disadvantages of library design issues.

Click here for a list of [EvaluatedLibraries evaluated libraries]

= Understanding Relative Runtime Plots =

Results are presented primarily using relative runtime plots.  These plots show fast one library runs relative to another library across a range of matrix sizes.  An example of such a plot is shown below:

http://java-matrix-benchmark.googlecode.com/svn/wiki/RuntimeQ9400_2010_01_22.attach/det.png

The x-axis is a function of the size of the input matrices and the y-axis shows how fast one library ran relative to reference library.  Larger the relative runtime is the faster the library is.  If a plot ends prematurely for a library that means that it either became too slow or a fatal error occurred and the test was stopped.

By viewing these plots it is easy to tell how fast a particular library runs relative to any other library.  Since these plots show the results as a function of matrix size it is often easy to see when CPU cache size starts adversely effecting one algorithm or when a library switches to a multi-threaded approach.  In the above plot it is easy to see where EJML uses specialized code for smaller matrices, before it switches to more generalized routines for larger ones. 

= Evaluated Operations =

Three primary types of operations are tested; basic, solving, and decompositions.  If a library did not support one of the operations it was simply omitted from that test.

== Basic Operations ==

The following are several common operations used in linear algebra.  There are many different permutations on these that some libraries support.  For sake of brevity, only a few have been tested.

The transpose then multiply test is included to allow libraries that do not do a physical transpose to demonstrate their transpose performance.  In general square matrices are used to avoid biasing the results towards an internal row-major or column-major format due to cpu caching issues.

|| Operation || Description || Matrix Dimension ||
|| {{{C = Î± * B}}} || Scaling || B = (m x m)||
|| {{{C = A + B}}} || Addition || A = (m x m), B=(m x m)||
|| {{{C = A * B}}} || Matrix multiplication. || A = (m x m), B=(m x m)||
|| C = A<sup>T</sup> {{{*}}} B || Transpose then multiplication. || A = (m x m), B=(m x m) ||
|| det(A) || Determinant || A = (m x m) ||
|| C = A<sup>T</sup> || Physical transpose. || A = (m x m) ||
|| C = inv(A) || Invert. || A = (m x m)||

== Linear Solving ==

Most libraries provided ways to solve for linear systems.  Typically there are different algorithms used when a square system is being solved for versus an overdetermined system.  Thus there are two benchmarks.

|| Operation || Description || Matrix Dimension ||
|| x = A<sup>-1</sup>b where m=n|| Solving for x when A is a square non-singular matrix.|| A = (m x m), b = (m x 2m)||
|| x = A<sup>-1</sup>b where m>n|| Solving for x when it is an overdetermined system. || A = (3m x m), b = (3m x 2m ||

== Decompositions ==

After each library performs a decomposition the decomposed matrices are then requested.  *NOTE: this is only done for SVD and eigen right now.*

|| Operation || Description ||
|| LU || Square non-singular matrices. ||
|| Cholesky || Square positive definite matrices. ||
|| QR || Rectangular matrices where m > n ||
|| SVD || Square matrix. ||
|| Eigen || Symmetric square matrix. ||