#summary Summary of performance benchmarks.

= Introduction =

The following is a summary what was evaluated and why.

= Java Matrix Benchmark Settings =

This is how the benchmark was configured.  See the code for an explanation of these settings.

|| Setting || Value ||
|| numBlockTrials || 5 ||
|| numBlocks || 5 ||
|| TRIAL_TIME || 3000 ||
|| memorySlaveBase || 10 ||
|| memorySlaveScale || 8 ||
|| randizeOrder || true ||

= Libraries Summary =

Below is a summary of each of the libraries that were tested.  The columns indicates features of each libraries implementations that are important for understanding the results.

|| Name || Version         || Multi-Threaded || Physical <br> Transpose || Cached <br> Results || In-Place ||
|| EJML ||  0.7            ||       || True  ||      ||      ||
|| Colt ||  1.2            || True  ||       ||      || True ||
|| Commons Math || 2.0     ||       || True  ||      ||      ||
|| Jama || 1.0.2           ||       || True  ||      ||      ||
|| JScience || 4.3         || True  ||       ||      ||      ||
|| MTJ || 0.9.12           ||       || True  ||      || True ||
|| OjAlgo || 28.26         || True  ||       || True ||      ||
|| Parallel Colt || 0.9.1  || True  ||       ||      || True ||
|| UJMP || 0.2.3           ||       || True  ||      ||      ||

== Benchmark/Library Design Issues ==

One problem that is encountered when benchmarking linear algebra libraries is that they often do not implement the exact same operation.  Here are three issues common issues 1) transpose, 2) caching, 3) partial solutions, 4) in-place operations.

=== Transpose ===
Libraries that work with accessors instead of the raw data can simply mark a matrix as being transposed.  When a get(x,y) function is called it internally swaps the x and y if it has been flagged. A library that physically transposes a matrix moves the memory around and no check needs to be done to see if it has been transposed.  For the transpose benchmark, libraries that do not perform a physical transpose are omitted.

=== Caching ===
If a matrix has not changed then there is no need to recompute expensive operations on that matrix.  This can make things simpler for a developer, but increases the memory requirements of the library and its complexity.  It can also wreck havoc on benchmarks, since what is supposed to be measured is its performance at computing the operation, not caching its results.  

The work around for this issue is to tell the library to forget its past results.  Which is accomplished by either telling it to flush its cache or creating a copy of the original matrix.  For small matrices this might effect the results, for larger ones the results should be unchanged.

=== Partial Results ===
When computing a decomposition of a matrix not all the results are needed all the time or the results need to be extracted from a partial solution.  Many of the libraries will postpone some computations until the full solution has been requested.  As a way around this issue the full decomposition is requested from all libraries.  This will not bias the results, but libraries with this feature might run faster in real-world applications.

=== In-Place ====
An in-place operation is an operation which modifies one of the original matrices instead of storing the results in another matrix.  Most libraries provide methods for the later, which is what this benchmark evaluated.  If a library only provides an in-place method, then a copy of the original matrix is created.  This will degrade performance for small matrices.  If a library is flagged as in-place above, then for at least one operation only an in-place variant was provided.

Effected operators are 'scale' and 'add' in Colt, Parallel Colt and MTJ libraries.

= Operations Tested =

Three primary types of operations are tested; basic, solving, and decompositions.  If a library did not support one of the operations it was simply omitted from that test.

== Basic Operations ==

The following are several common operations used in linear algebra.  There are many different permutations on these that some libraries support.  Testing them all is not of practical value.

Matrix transpose is primarily handled in two ways by the libraries.  In some it is a physical transpose where a new matrix is created and the transpose written to it.  In others a matrix is just flagged as being transposed.  The transpose benchmark only tested a physical transpose.  However, to test the merits of flagging a library as being transposed, a transpose then multiplication operation was also tested.

|| Operation || Description ||
|| {{{C = Î± * B}}} || Scaling ||
|| {{{C = A + B}}} || Addition ||
|| {{{C = A * B}}} || Matrix multiplication ||
|| C = A<sup>T</sup> {{{*}}} B || Transpose then multiplication. ||
|| det(A) || Determinant ||
|| C = A<sup>T</sup> || Physical transpose ||
|| C = inv(A) || Invert ||

== Linear Solving ==

Most libraries provided ways to solve for linear systems.  Typically there are different algorithms used when a square system is being solved for versus an overdetermined system.  Thus there are two benchmarks.

|| Operation || Description ||
|| {{{A*x = b}}} where m=n|| Solving for x when A is a square non-singular matrix.||
|| {{{A*x = b}}} where m>n|| Solving for x when it is an overdetermined system.||

== Decompositions ==

Internally how each library performed decompositions varied significantly.  For example,
some out extract all the decomposed matrices at once, while others would only extract
them upon request.  To overcome this problem all of the decompositions tests would first
request that they decompose the matrix, then extract each of the decomposed matrices.

One could argue that extracting the decomposed matrices is often an unnecessary.  While
true, one would also need to understand the internals of how the algorithm worked to avoid doing so.  

|| Operation || Description ||
|| LU || Square non-singular matrices. ||
|| Cholesky || Square positive definite matrices. ||
|| QR || Rectangular matrices where m > n ||
|| SVD || Square matrix. ||
|| Eigen || Symmetric square matrix. ||