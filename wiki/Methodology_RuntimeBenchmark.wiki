#summary Description of the runtime benchmark methodology and justifications.

= Introduction =

In principle benchmarking an algorithm on a computer is a simple task.  For this project we wanted measure how fast various matrix libraries are at performing different linear algebra operations across a variety of matrix sizes.  A very simple benchmark for this task might go something like:

{{{
    public void performBenchmark( Libraries libraries ) {
        for( Libraries lib : libraries ) {
            for( Operation op : lib.getOperations() ) {
                for( int matrixSize = 0; matrixSize < maxSize; matrixSize++ ) {
                    long timeBefore = System.currentTimeMillis();
                    
                    op.performTest( matrixSize , numberOfTrials );
                    
                    long timeAfter = System.currentTimeMillis();
                    
                    saveResults( lib , op , matrixSize , timeAfter-timeBefore );
                }
            }
        }
    }
}}}

This approach is very simple and for many application is good enough, when only a crude estimate is needed.  Unfortunately it will produce inaccurate results in practice because it fails to take in account several factors:

  # Implicit assumptions about the system it is run on.
  # Variability of results depending on when they were run.
  # Influence of previous history on current results.
    # Libraries can directly effect results of other libraries.
    # Runtime optimization performed by the JavaVM.
  # Accidental overloading of library classes.
  # Benchmark's overhead.

In addition how the statistics are computed make the big difference.  Are the worst case results the most interesting?  The best case?  How about the median or the average?  What do each of these metrics tell you?

In the following subsection these issues are discussed and examples given to illustrate their importance.

= 1 Benchmark Procedures =

Measuring runtime performance accurately requires procedures that carefully take in account the reality of how modern computers operate. Desktop computers today are very complex and no single application has control over the entire system.  This complexity adds more unknowns and makes it more difficult to predict future performance based upon your past results, which is what a benchmark is used to do.  

Benchmarking Java applications is even more difficult than native applications because of the additional layer of abstraction from the hardware.  Even a simple application will show more variability in its runtime than a simple application written in c when run multiple times.  Modern Java virtual machines (VM) will optimize the code at runtime.  Making it even more difficult to predict how they will run in the future.  

== 1.1 Implicit Assumptions ==

When writing a benchmark it is hard if not impossible to not make implicit assumptions about the system the benchmark is run on.  The most common culprit is the variable 'numberOfTrials' that determines the number of times a particular trial is run.  If set too small then the performance can not be accurate measured.  If set too high then it will take too long. For example, if a benchmark is written on a very fast computer then in might be too slow when used on an older computer.  Then years later when it is run on a new computer it might be too slow.

A benchmark running too fast leads to inaccurate results because there is always noise in how its runtime is measured.  Mathematically this can be viewed as a random process:
{{{
t_m = t_0 + N(t_0)
}}}
where 't_m' is the measured time, 't_0' is the true time, and N(t_0) is a noise process that may be dependent on when it is called.  This issue is exaggerated in Java due to the abstraction from the systems hardware, preventing very low level system calls from being called.

In Java it has been long known that System.currentTimeMillis() has an accuracy within milliseconds, but can be variable depending on which operations/hardware it is run on.  System.nanoTime() returns a more precise time measurement, but its documentation explicitly states that its accuracy is not defined, see below:
{{{
This method can only be used to measure elapsed time and is not related to any other.
This method provides nanosecond precision, but not necessarily nanosecond accuracy. 
}}}

One way to handle this problem is to simply increase the length of the benchmark until the noise is insignificant.  As previously mentioned the number of trials that this requires is dependent on the system it is run on.  

In this benchmark matrices of different sizes are tested.  In general, the larger the matrix is the longer it takes to process.  Determining the maximum matrix size has similar issues to determining the number of trials and is system dependent.

=== 1.1.1 The Java Matrix Benchmark Approach ===

JMatBench dynamically determines the number of trials by estimating how long each operation takes.  Each time a new benchmark is run it first runs a small number of trials.  Then it estimates how long it takes to run each operation and increases the number of trials so that it should last for a minimum duration T<sub>min</sub>.  The process is repeated until it approximately runs for T<sub>min</sub> seconds.  At which point the trial stops and the number of operations per second is returned.

The size of the matrices that it processes is also dynamically determined at runtime.  This is done simply by having a maximum amount of time allowed for each test.  If that time is exceeded the test stops.  A maximum matrix size specified to prevent it from running for too long or beyond any interesting point.  A test will be killed if it is taking too long.

A side effect of this approach is that the amount of time it takes for a complete benchmark to run is approximately system independent.  It would take about as long on a state of the hard computer 10 years from now as it would on old computer collecting dust.

== 1.2 Influence of Past History ==

A basic assumption made when measuring an algorithm's performance is that its current performance is independent of what was run before it.  If this assumption is broken then results become unpredictable or unreproducible on different systems. At a glance the sample code above wouldn't have this issue since by the time new library is being evaluated the old library would have stopped and would no longer be running. Unfortunately this is not the case.  A few of the reasons why this is so are listed below:

  # The previous library or operation could have left stray threads around.
  # The Java garbage collector might have not had run yet in the previous test and so it runs in the current one.
  # Two libraries might have different classes with the same name and classpath.
  # The virtual machine might have optimised itself for a different configuration and might be a suboptimal state for the current one.

1) With concurrent (multithreaded) algorithms becoming more and more popular as multiprocessor/multicore systems become mainstream problems associated with them will increase.  Writing good multithreaded is not easy and there are several common errors that can significantly effect other tests performed later on.  For example, a deadlock condition is when a thread is waiting for something to happen that never will happen.  The thread might never stop consuming computation and memory resources until it is killed by the application exiting.

2) Java applications have very little control over when the garbage collected might run.  They can request that it run at a particular time, but they can not force it to run.  It is possible for one test to consume a lot of memory only to have the next test take a performance hit when the garbage collector run.  How the garbage collector behaves is highly dependent on its own algorithm, which constantly change with each new release of the JavaVM.

3) It is possible for two libraries to have a class with exactly the same class name and package but have different behavours.  If this happens and both libraries are tested inside the same application instance then Java will override one library's classes with the others at runtime silently making the benchmark's results invalid.  

While this is unlikely it is possible.  Some libraries are derived from other libraries, share a common ancestry, or maybe the authors just think like.  There isn't any known cases of this problem existing since most authors change the name of a class when they absorb a class from another project.

4) To most people how the JavaVM performs its runtime optimization is unknown and a bit of a mystery.  One reason is that the developers are constantly changing the algorithms to improve its performance.  The typical assumption is that the first time some code is run it will be slow then the second time it is run it will be much faster.  Based on experience the true situation is much more confusion and difficult to understand.

To illustrate how significant this issue is consider the following benchmark that compares the performance of three different ways to multiply matrices:
{{{
public class BenchmarkMatrixMatrixMult {

    public static long multSmall( DenseMatrix64F matA , DenseMatrix64F matB ,
                             DenseMatrix64F matResult , int numTrials) {
        long prev = System.currentTimeMillis();

        for( int i = 0; i < numTrials; i++ ) {
            MatrixMatrixMult.mult_small(matA,matB,matResult);
        }

        return System.currentTimeMillis()-prev;
    }

    public static void main( String args[] ) {
        int size[] = new int[]{2,4,10,20,50,100,200,500,1000};
        int count[] = new int[]{30000000,6000000,600000,140000,15000,1500,150,8,2,1,1};

        for( int i = 0; i < size.length; i++ ) {
            int width = size[i];
            DenseMatrix64F matA = RandomMatrices.createRandom(width,width,rand);
            DenseMatrix64F matB = RandomMatrices.createRandom(width,width,rand);
            DenseMatrix64F matResult = RandomMatrices.createRandom(width,width,rand);

            long timeSmall = multSmall(matA,matB,matResult,count[i]);
            long timeAux = multAux(matA,matB,matResult,count[i]);
            long timeLarge = multLarge(matA,matB,matResult,count[i]);

            System.out.printf("Size %4d elapsed time: %6d %6d %6d\n",width,timeSmall,timeAux,timeLarge);
        }
    }
}
}}}
Please note that the code has been truncated to only show what is essential.  When the above code is run on JavaVM 1.6.0_15 with my computer it produces the following output:
{{{
Size    2 elapsed time:   3766   4165   3990
Size    4 elapsed time:   3445   2817   3669
Size   10 elapsed time:   3887   2720   3969
Size   20 elapsed time:   6425   4276   6577
Size   50 elapsed time:   9931   6419  10412
Size  100 elapsed time:   8250   5391   8417
Size  200 elapsed time:   6384   4131   6317
Size  500 elapsed time:   9146   3383   5175
Size 1000 elapsed time:  24195   7191  10677
}}}

Now let's say that you are only concerned with the results for matrices that are 200 or above.  So you simply modify the code so that 'i' is set to 6 initially and you get this output:
{{{
Size  200 elapsed time:   1798   2316   2198
Size  500 elapsed time:   7556   1941   1855
Size 1000 elapsed time:  13574   3562   3491
}}}

The results are now completely different!  Why did this happen?  That's hard to say, but clearly the conclusions you reach are dependent upon the order in which you process your results.

=== 1.2.1 The Java Matrix Benchmark Approach ===

== 1.3 Accidental Overloading ==

= 2 Benchmark Statistics =