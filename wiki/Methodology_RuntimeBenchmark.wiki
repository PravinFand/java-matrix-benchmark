#summary Description of the runtime benchmark methodology and justifications.

= Introduction =

In principle benchmarking an algorithm on a computer is a simple task.  For this project we wanted measure how fast various matrix libraries are at performing different linear algebra operations across a variety of matrix sizes.  A very simple implementation of a benchmark for this task might go something like this:

{{{
    public void performBenchmark( Libraries libraries ) {
        for( Libraries lib : libraries ) {
            for( Operation op : lib.getOperations() ) {
                for( int matrixSize = 0; matrixSize < maxSize; matrixSize++ ) {
                    long timeBefore = System.currentTimeMillis();
                    
                    op.performTest( matrixSize , numOfTrials );
                    
                    long timeAfter = System.currentTimeMillis();
                    
                    saveResults( op , timeAfter-timeBefore );
                }
            }
        }
    }
}}}

This approach is very simple and for many application is good enough when only a crude estimate is needed.  Unfortunately it will produce inaccurate results in practice because it fails to take in account several factors:

  # Implicit assumptions about the system it is run on.
  # Variability of results depending on when they were run.
  # Influence of previous history on current results.
    # Libraries can directly effect results of other libraries.
    # Runtime optimization performed by the JavaVM.
  # Accidental overloading of library classes.

In addition how the statistics are computed make the big difference.  Are the worst case results the most interesting?  The best case?  How about the median or the average?  What do each of these metrics tell you?

In the following subsection these issues are discussed and examples given to illustrate their importance.

= Benchmark Procedures =

How accurately the runtime performance is measured is entirely dependent on carefully taking in account factors of modern computers. Desktop computers today are very complex and no single application has control over the entire system.  This complexity adds more unknowns and makes it more difficult to predict future performance based upon your past results, which is what a benchmark is used to do.  

Benchmarking Java applications is even more difficult than native applications because of the additional layer of abstraction from the hardware.  Even a simple application will show more variability in its runtime than a simple application written in c.  Modern Java virtual machines (VM) will optimize the code at runtime.  

== Implicit Assumptions ==


== Past History ==

== Accidental Overloading ==

= Benchmark Statistics =
